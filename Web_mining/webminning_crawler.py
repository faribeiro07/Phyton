# -*- coding: utf-8 -*-
"""WebMinning_Crawler

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qzMFMzF1YfxKJcq3916aDxh6vwMKkRgq

WEBMINING | CRAWLER

Composto por cinco etapas:
1.   Escolha um link da web para iniciar o Crawler
2.   Adicione em uma lista de links para capturar 
3.   Enquanto a lista não tiver vazia:
  4.   Pegue um link da lista
  5.   Se o link ainda não foi capturado
      6.   Capture o conteúdo HTML deste link
      7.   Armazene o conteúdo capturado
      8.   Armazene o link na lista de capturados
      9.   Procure os links existentes no conteúdo deste link
      10.   Armazene os links encontrados  na lista capturar
"""

import requests
from requests.compat import urljoin
from bs4 import BeautifulSoup

import re, os, IPython

#Escolhendo uma URL
#Requests é a bibliteca que irá carregar uma página
init = 'https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial'
init2 = 'https://spoilermovies.com.br/'

response = requests.get(init) #verifica se a página está ativa
response.status_code #Retorno 200 informa que houve sucesso para carregar a página

#Verificar o conteúdo HTML da página selecionada
html = response.text #text é código que carrega o texto
html

#Comando para interpretar o conteúdo da página carreda
bs =  BeautifulSoup(html, 'html.parser') #Parametro html.parser identifica o tipo de dado que será lida
bs

#Como filtrar elementos html do conteúdo
links = bs('a') #filtro do elemento a que identifica links.
links

#Coletando Atributos
hrefs = [l.get('href') for l in links] #irá retornar cada atributo presente no conteúdo da página  

#o comando l.get('href') for l in links é a simplificação do laço abaixo:
#href = [] #Criada lista vazia
#for l in links: #para cada linha dentro da lista de Links
#  hrefs.append(l.get('href')) #o dado será adicionado a lista de href criada acima, e o valor de get

hrefs[:10]

#Construindo o Crawler
caminho_base = '/content/drive/MyDrive/web-mining/Database' #caminho que salvará os resultados do crawler

def links_validos(href): #formula para filtrar links indesejados
  if href != None and href != '' and href[0] != "#": #no caso pula os links inexistentes, vazios ou ãncoras
    return True
  return False

def busca_links(conteudo, link): #função para buscas os links
  bsObj = BeautifulSoup(conteudo, 'html.parser') #executa a leitura com interpretação html
  links = bsObj('a', href=links_validos) #pesquisa pelos marcadores de link 'a' e filtra os links indesejados
  links = [urljoin(link, l.get('href')) for l in links] #urljoin concatena o link da página + link extraído. Segundo comando faz a varredura de cada link da página
  return links 

#função para salvar arquivo resultante do crawler
def salva_arquivo(pasta,nome,conteudo):

  if not os.path.isdir(pasta): #Verifica se a pasta existe
    os.makedirs(pasta, exist_ok =True) #Caso não ele cria uma e o segundo parâmetro evita que seja subscrita

  with open(f'{pasta}/{nome}', 'w', encoding='utf8') as arq: #abre o arquivo pasta/nome, forçando escrita e lendo com acentuações
    arq.write(conteudo) #efetua a escrita no arquivo

#método contempla três fases:
#Buscar. Parametro obrigatório nesta função
#Buscado. Parametro obrigatório nesta função
#stop = limite de buscas. O número adicionado ao paramêtro sinaliza que ele é opcional

def crawler(buscar, buscados, stop = 1):
  if buscar and len(buscados) < stop: #defini critério de encerramento do crawler
    link = buscar.pop() #caso precise rodar, ele removerá o link da lista de buscar
    if link not in buscados: #verifica se o link está presente na lista de buscados
      print(f'Baixando {link}')
      response = requests.get(link) #caso não esteja, ele adicona na lista de buscados
      salva_arquivo(
          pasta = f'{caminho_base}/wiki',
          nome = f'wiki{len(buscados)+1}.txt',
          conteudo = response.text
      ) #dentro da função do crawler, estamos ativando a função salva arquivo e definindo os três argumentos: pasta, nome e conteudo
      buscados.add(link) # adiciona o link na lista de buscados
      novos_links = busca_links(response.text, link) #pesquisará por novos links
      buscar.update(novos_links) #atualiza a lista de novos_links na lista de buscar para continuar a lista
    crawler(buscar, buscados, stop) #Chama a função crawle de novo para repetir o processo enquanto houver links para serem buscados

init = 'https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial'
crawler(set([init]),set([]),5) #o set() é usado, pois cria conjunto que permite dados repetidos. Listas [] não

